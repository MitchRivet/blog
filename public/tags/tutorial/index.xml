<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mitch Rivet</title>
    <link>www.mitchrivet.com/tags/tutorial/index.xml</link>
    <description>Recent content on Mitch Rivet</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="www.mitchrivet.com/tags/tutorial/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Analyzing Reddit Comments: Scraping (Part 1)</title>
      <link>/www.mitchrivet.com/analyzing-reddit-comments-scraping-part-1</link>
      <pubDate>Sun, 06 Nov 2016 22:17:00 +0200</pubDate>
      
      <guid>/www.mitchrivet.com/analyzing-reddit-comments-scraping-part-1</guid>
      <description>

&lt;p&gt;A place to start when getting into data science is to find data sets that are personally relevant and/or interesting. one place i like to get data to understand more about internet discourse is reddit.&lt;/p&gt;

&lt;h2 id=&#34;how-to-scrape-reddit&#34;&gt;How to scrape reddit&lt;/h2&gt;

&lt;p&gt;if we want to download data from reddit, we essentially have two options. one is making an oauth request which includes logging into the reddit api with a reddit account. the downside to this method is that each user is limited to 60 oauth requests per hour, which makes it hard to download an entire comment thread and all of the relevant user meta data from comments in the thread (popular discussions usually break over 1,000 comments).&lt;/p&gt;

&lt;p&gt;another option is to scrape reddit, or in other words use a server to visit web pages and actually download the html content we&amp;rsquo;re looking for ourselves, which we can then parse into whatever relevant bits we are looking for. reddit makes this process even one step easier by exposing most of their endpoints as json strings. we can see this by going to any thread on reddit and attaching a &amp;lsquo;/.json&amp;rsquo; to the end of the url. this allows us to easily convert these giant strings straight to json without having to create our own structure. the downside of this method is that we sometimes end up with incomplete data sets. consider the links on reddit which say &amp;lsquo;more comments&amp;rsquo; -&amp;gt; by scraping reddit our data will include this &amp;lsquo;more comments&amp;rsquo; link without us being able to access that data directly. we could tell our server to collapse all those links, but lets just start with a general data set to get going.&lt;/p&gt;

&lt;p&gt;lets use node and expressjs to do this. we start by npm installing and setting up a basic expressjs app. expressjs is a routing framework for nodejs, a javascript development environment for creating servers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;

    //declare our initial dependencies
    var express = require(&#39;express&#39;);
    var request = require(&#39;request&#39;);

    //initialize express
    var app = express();

    app.get(&#39;/scrape&#39;, (req, res) =&gt; {
        res.json({message: &#39;this is your endpoint&#39;});
    });

    app.listen(&#39;8081&#39;);     

    console.log(&#39;Listening on port 8081&#39;);
    exports = module.exports = app;

&lt;/pre&gt;&lt;/code&gt;


lets save this file as &#39;reddit_request.js&#39;. we can now fire up our server by opening the command line into the directory where we saved this file and running:

&lt;pre&gt;&lt;code&gt;node reddit_request.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thanks to our console.log, we should get a message printed in the terminal saying &amp;lsquo;Listening on port 8081&amp;rsquo;. If everything is working correctly, we should be able to open our web browser to localhost:8081/scrape and see the json we returned fromt that endpoint.&lt;/p&gt;

&lt;p&gt;if that&amp;rsquo;s all working, we have just created a server with an endpoint called &amp;lsquo;/scrape&amp;rsquo;. when use express to do our routing, we can use our client to make a get request to the server.&lt;/p&gt;

&lt;h2 id=&#34;making-requests&#34;&gt;Making Requests&lt;/h2&gt;

&lt;p&gt;Now lets use the request module to download the content we want from Reddit. Request is a built in NodeJS module and can be used to make HTTP and HTTPS requests, allowing us to download html or in this case, JSON strings. We are going to modify our endpoint like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;

    //
    //

    app.get(&#39;/scrape&#39;, (req, res) =&gt; {
        var url = &#39;https://www.reddit.com/r/politics/comments/5bu1u6/if_you_vote_for_trump_you_own_the_racism_yes_you/.json&#39;;

        //note we won&#39;t be using the html return value in the callback, but I included it so you would know it&#39;s there
        request(&#39;url&#39;, (err, response, html) =&gt; {
            if (err) {
                console.log(err);
            } else {
                let commentJSON = JSON.parse(response.body);
                res.json(commentJSON);
            }
            });
    });

    //
    //

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, reload the server with the &amp;lsquo;node reddit_request.js&amp;rsquo; command, make the same request to localhost:8081/scrape in your client. you should see a giant mess of json that is a replica of whatever thread you were looking at. I recommend using chrome and a chrome module to make the JSON collapsable and much more readable.&lt;/p&gt;

&lt;h2 id=&#34;parsing-comments&#34;&gt;Parsing Comments&lt;/h2&gt;

&lt;p&gt;Notice that the structure of the comments is nested in the same way they are visible on the html page. what if we want to analyze each comment individually? we would want to place each comment into a flat array. if we want to get a structure like this we will need to use recursion. We essentially want the computer to go through each comment and ask &amp;ldquo;does this comment have any replies? if yes, then i will run the same operation of the array of comments inside of the reply property. if not, then i will continue processing the top level of the comments array&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;For this task, we can use a new syntax to es6 called generators. Add the following function globally, above your &amp;lsquo;/scrape&amp;rsquo; endpoint. We will still need to call the function later on, but lets first define and understand it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;
    //
    //

    //we&#39;re passing in all of our returned comments
    function *findReplies(comments) {
        if (!comment) {return ;}

        //looping through all of the comments
        for (var i = 0; i &lt; comments.length; i++) {

            //which ever step of the array we are on, grab the data
            var commentMeta = comments[i].data;

            //checking that the comment was not deleted    
            if (commentMeta.body &amp;&amp; commentMeta.author !== &#34;[deleted]&#34;) {

              allWordsString = allWordsString + commentMeta.body + &#39;&#39;;
              var score = sentiment(commentMeta.body);

              yield {
                author: commentMeta.author,
                body: commentMeta.body,
                score: score
              };
            }

            if (commentMeta.replies) {
              yield *findReplies(commentMeta.replies.data.children);
            }
          }
    }

    app.get(&#39;/scrape&#39;, (req, res) =&gt; {

    //
    //
&lt;/code&gt;&lt;/pre&gt;  

&lt;p&gt;Every time we hit a yield statement in this function, whatever we yield will be part of a larger return. This happens in two places: the first when we grab the comment meta data we are looking for, and the second when we call the generator itself if the comment contains comments within. A function calling itself is &lt;a href=&#34;https://en.wikipedia.org/wiki/Recursion&#34;&gt;Recursion&lt;/a&gt;. This syntax yields itself nicely to what we are trying to do.&lt;/p&gt;

&lt;p&gt;Now, how do we call this function?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;
//
//

app.get(&#39;/scrape&#39;, (req, res) =&gt; {

    var allComments = [];

    var url = &#39;https://www.reddit.com/r/politics/comments/5bu1u6/if_you_vote_for_trump_you_own_the_racism_yes_you/.json&#39;;

    request(&#39;url&#39;, (err, response, html) =&gt; {
        if (err) {
            console.log(err);
        } else {
            let commentJSON = JSON.parse(response.body);
            let header = response.body[0];
            let comments = commentJSON[1].data.children;

            var getReply = findReplies(comments);
            var replyGrab = getReply.next();

            //the generator has a state that we can manage inside of a while statement
            while (!replyGrab.done) {
              allComments.push(replyGrab.value);
              replyGrab = getReply.next();
            }

            //once the generator is finished, we can send our parsed data back to the client
            if (replyGrab.done) {
                res.json({header: header, comments: allComments });
            }
        }
        });
});
//
//
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, open your browser again and double check that the data is being sent back. In next installments, we will take a lot at asynchronously fetching user comments, other options for language processing, and creating a ReactJS client to visualize data.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>