<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Post-rsses on Mitch Rivet</title>
    <link>www.mitchrivet.com/post/index.xml</link>
    <description>Recent content in Post-rsses on Mitch Rivet</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 06 Nov 2016 22:17:00 +0200</lastBuildDate>
    <atom:link href="www.mitchrivet.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Analyzing Reddit Comments: Scraping (Part 1)</title>
      <link>/www.mitchrivet.com/analyzing-reddit-comments-scraping-part-1</link>
      <pubDate>Sun, 06 Nov 2016 22:17:00 +0200</pubDate>
      
      <guid>/www.mitchrivet.com/analyzing-reddit-comments-scraping-part-1</guid>
      <description>

&lt;p&gt;A place to start when getting into data science is to find data sets that are personally relevant and/or interesting. one place i like to get data to understand more about internet discourse is reddit.&lt;/p&gt;

&lt;h2 id=&#34;how-to-scrape-reddit&#34;&gt;How to scrape reddit&lt;/h2&gt;

&lt;p&gt;if we want to download data from reddit, we essentially have two options. one is making an oauth request which includes logging into the reddit api with a reddit account. the downside to this method is that each user is limited to 60 oauth requests per hour, which makes it hard to download an entire comment thread and all of the relevant user meta data from comments in the thread (popular discussions usually break over 1,000 comments).&lt;/p&gt;

&lt;p&gt;another option is to scrape reddit, or in other words use a server to visit web pages and actually download the html content we&amp;rsquo;re looking for ourselves, which we can then parse into whatever relevant bits we are looking for. reddit makes this process even one step easier by exposing most of their endpoints as json strings. we can see this by going to any thread on reddit and attaching a &amp;lsquo;/.json&amp;rsquo; to the end of the url. this allows us to easily convert these giant strings straight to json without having to create our own structure. the downside of this method is that we sometimes end up with incomplete data sets. consider the links on reddit which say &amp;lsquo;more comments&amp;rsquo; -&amp;gt; by scraping reddit our data will include this &amp;lsquo;more comments&amp;rsquo; link without us being able to access that data directly. we could tell our server to collapse all those links, but lets just start with a general data set to get going.&lt;/p&gt;

&lt;p&gt;lets use node and expressjs to do this. we start by npm installing and setting up a basic expressjs app. expressjs is a routing framework for nodejs, a javascript development environment for creating servers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;

    //declare our initial dependencies
    var express = require(&#39;express&#39;);
    var request = require(&#39;request&#39;);

    //initialize express
    var app = express();

    app.get(&#39;/scrape&#39;, (req, res) =&gt; {
        res.json({message: &#39;this is your endpoint&#39;});
    });

    app.listen(&#39;8081&#39;);     

    console.log(&#39;Listening on port 8081&#39;);
    exports = module.exports = app;

&lt;/pre&gt;&lt;/code&gt;


lets save this file as &#39;reddit_request.js&#39;. we can now fire up our server by opening the command line into the directory where we saved this file and running:

&lt;pre&gt;&lt;code&gt;node reddit_request.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thanks to our console.log, we should get a message printed in the terminal saying &amp;lsquo;Listening on port 8081&amp;rsquo;. If everything is working correctly, we should be able to open our web browser to localhost:8081/scrape and see the json we returned fromt that endpoint.&lt;/p&gt;

&lt;p&gt;if that&amp;rsquo;s all working, we have just created a server with an endpoint called &amp;lsquo;/scrape&amp;rsquo;. when use express to do our routing, we can use our client to make a get request to the server.&lt;/p&gt;

&lt;h2 id=&#34;making-requests&#34;&gt;Making Requests&lt;/h2&gt;

&lt;p&gt;Now lets use the request module to download the content we want from Reddit. Request is a built in NodeJS module and can be used to make HTTP and HTTPS requests, allowing us to download html or in this case, JSON strings. We are going to modify our endpoint like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;

    //
    //

    app.get(&#39;/scrape&#39;, (req, res) =&gt; {
        var url = &#39;https://www.reddit.com/r/politics/comments/5bu1u6/if_you_vote_for_trump_you_own_the_racism_yes_you/.json&#39;;

        //note we won&#39;t be using the html return value in the callback, but I included it so you would know it&#39;s there
        request(&#39;url&#39;, (err, response, html) =&gt; {
            if (err) {
                console.log(err);
            } else {
                let commentJSON = JSON.parse(response.body);
                res.json(commentJSON);
            }
            });
    });

    //
    //

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, reload the server with the &amp;lsquo;node reddit_request.js&amp;rsquo; command, make the same request to localhost:8081/scrape in your client. you should see a giant mess of json that is a replica of whatever thread you were looking at. I recommend using chrome and a chrome module to make the JSON collapsable and much more readable.&lt;/p&gt;

&lt;h2 id=&#34;parsing-comments&#34;&gt;Parsing Comments&lt;/h2&gt;

&lt;p&gt;Notice that the structure of the comments is nested in the same way they are visible on the html page. what if we want to analyze each comment individually? we would want to place each comment into a flat array. if we want to get a structure like this we will need to use recursion. We essentially want the computer to go through each comment and ask &amp;ldquo;does this comment have any replies? if yes, then i will run the same operation of the array of comments inside of the reply property. if not, then i will continue processing the top level of the comments array&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;For this task, we can use a new syntax to es6 called generators. Add the following function globally, above your &amp;lsquo;/scrape&amp;rsquo; endpoint. We will still need to call the function later on, but lets first define and understand it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;
    //
    //

    //we&#39;re passing in all of our returned comments
    function *findReplies(comments) {
        if (!comment) {return ;}

        //looping through all of the comments
        for (var i = 0; i &lt; comments.length; i++) {

            //which ever step of the array we are on, grab the data
            var commentMeta = comments[i].data;

            //checking that the comment was not deleted    
            if (commentMeta.body &amp;&amp; commentMeta.author !== &#34;[deleted]&#34;) {

              allWordsString = allWordsString + commentMeta.body + &#39;&#39;;
              var score = sentiment(commentMeta.body);

              yield {
                author: commentMeta.author,
                body: commentMeta.body,
                score: score
              };
            }

            if (commentMeta.replies) {
              yield *findReplies(commentMeta.replies.data.children);
            }
          }
    }

    app.get(&#39;/scrape&#39;, (req, res) =&gt; {

    //
    //
&lt;/code&gt;&lt;/pre&gt;  

&lt;p&gt;Every time we hit a yield statement in this function, whatever we yield will be part of a larger return. This happens in two places: the first when we grab the comment meta data we are looking for, and the second when we call the generator itself if the comment contains comments within. A function calling itself is &lt;a href=&#34;https://en.wikipedia.org/wiki/Recursion&#34;&gt;Recursion&lt;/a&gt;. This syntax yields itself nicely to what we are trying to do.&lt;/p&gt;

&lt;p&gt;Now, how do we call this function?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;
//
//

app.get(&#39;/scrape&#39;, (req, res) =&gt; {

    var allComments = [];

    var url = &#39;https://www.reddit.com/r/politics/comments/5bu1u6/if_you_vote_for_trump_you_own_the_racism_yes_you/.json&#39;;

    request(&#39;url&#39;, (err, response, html) =&gt; {
        if (err) {
            console.log(err);
        } else {
            let commentJSON = JSON.parse(response.body);
            let header = response.body[0];
            let comments = commentJSON[1].data.children;

            var getReply = findReplies(comments);
            var replyGrab = getReply.next();

            //the generator has a state that we can manage inside of a while statement
            while (!replyGrab.done) {
              allComments.push(replyGrab.value);
              replyGrab = getReply.next();
            }

            //once the generator is finished, we can send our parsed data back to the client
            if (replyGrab.done) {
                res.json({header: header, comments: allComments });
            }
        }
        });
});
//
//
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, open your browser again and double check that the data is being sent back. In next installments, we will take a lot at asynchronously fetching user comments, other options for language processing, and creating a ReactJS client to visualize data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why Blog?</title>
      <link>/www.mitchrivet.com/why-blog</link>
      <pubDate>Thu, 03 Nov 2016 22:17:00 +0200</pubDate>
      
      <guid>/www.mitchrivet.com/why-blog</guid>
      <description>

&lt;p&gt;Here are the practical reasons I see for maintaining a blog:&lt;/p&gt;

&lt;h2 id=&#34;1-generate-better-ideas&#34;&gt;1. Generate better ideas&lt;/h2&gt;

&lt;p&gt;Although internet criticism can be exceedingly negative, there is a lot of important feedback to be gained from opening your projects, plans, and thoughts to the online world. How can you assume that your ideas are worth anything without allowing them to be tested and critiqued? Writing also forces us to clarify our thinking, truly owning our ideas. Blogging can help you step out of your comfort zone into the arena of public discourse.&lt;/p&gt;

&lt;h2 id=&#34;2-help-others-by-sharing-what-you-learn&#34;&gt;2. Help others by sharing what you learn&lt;/h2&gt;

&lt;p&gt;Even though there are tons of online resources for learning programming topics, there is always value in sharing tutorials for niche subjects and explaining ideas using your own analogies and mental processes. I have often found explanations shared to me by my peers to be the most memorable. It&amp;rsquo;s also commonly said that the best way to learn something yourself is to &lt;a href=&#34;http://www.futurity.org/learning-students-teaching-741342&#34;&gt;teach others&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;3-improve-your-process-by-regularly-collecting-data&#34;&gt;3. Improve your process by regularly collecting data&lt;/h2&gt;

&lt;h2 id=&#34;4-create-social-accountability&#34;&gt;4. Create social accountability&lt;/h2&gt;

&lt;h2 id=&#34;5-if-your-work-is-not-online-it-doesn-t-exist&#34;&gt;5. &amp;ldquo;If your work is not online, it doesn&amp;rsquo;t exist&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;Taken from the lovely book [&amp;ldquo;Show Your Work!&amp;rdquo; by Austin Kleon]()&lt;/p&gt;

&lt;h2 id=&#34;6-give-others-insight-into-what-you-are-doing-right-now&#34;&gt;6. Give others insight into what you are doing right now&lt;/h2&gt;

&lt;p&gt;I plan to include software development tutorials, notes from the topics I&amp;rsquo;m currently studying, as well as essays about music and audio production. Whenever I finish a book, I will also share the notes I took and my opinion of it, inspired by &lt;a href=&#34;https://sivers.org/book&#34;&gt;Derek Sivers&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>